# 数据挖掘
## 概念

### 聚类
    软聚类：一个元素可以属于多个类，如好坏公司等  
    硬聚类：一个元素之能属于一个类，如奇偶数  
    
如果预测目标是离散的，则为分类，如果是连续的，则为回归

简单地说，分类(Categorization or Classification)就是按照某种标准给对象贴标签(label)，再根据标签来区分归类。
简单地说，聚类是指事先没有“标签”而通过某种成团分析找出事物之间存在聚集性原因的过程。

回归其实就是对已知公式的未知参数进行估计  
逻辑回归用在二值预测，比如预测一个客户是否会流失，只有0-不流失，1-流失；线性回归用来进行连续值预测，比如预测投入一定的营销费用时会带来多少收益。
逻辑回归的值不是概率，是归一化的结果，归一化是为了各个attribute可以同比，逻辑回归就是一个被logistic方程归一化后的线性回归，仅此而已
个attribute之间不需要满足条件独立假设，但是是独立计算的
朴素贝叶斯需要满足条件独立假设

先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料

在众多的分类模型中，应用最为广泛的两种分类模型是决策树模型(Decision Tree Model)和朴素贝叶斯模型（Naive Bayesian Model，NBC）。决策树模型通过构造树来解决分类问题。首先利用训练数据集来构造一棵决策树，一旦树建立起来，它就可为未知样本产生一个分类。在分类问题中使用决策树模型有很多的优点，决策树便于使用，而且高效；根据决策树可以很容易地构造出规则，而规则通常易于解释和理解；决策树可很好地扩展到大型数据库中，同时它的大小独立于数据库的大小；决策树模型的另外一大优点就是可以对有许多属性的数据集构造决策树。决策树模型也有一些缺点，比如处理缺失数据时的困难，过度拟合问题的出现，以及忽略数据集中属性之间的相关性等。 和决策树模型相比，朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同时，NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，NBC模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给NBC模型的正确分类带来了一定影响。在属性个数比较多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好

分类是用来预测类标号
预测用来估计连续值

加权评分表(Weighted Score table):   对各个属性如年龄收入有不同的权重,然后每个属性值有不同分数,加权得出的一个数字,可用于得到客户信用评分等



### 预处理：
取样：前Ｋ个，后Ｋ个，随机等
方差检验：

### 异类检测
Amornal detection: 利用K-means做异类检测，G-means来确定有多少组

### Apriori
关联分析,根据统计概率来计算可能性  
X&Y=>Z  
支持度：有多少记录满足此规则  
置信度：包含X,Y的交易中包含Z的概率，可以误导，如吃大米的都打篮球等  
提升度：解决上面置信度的问题  

### K-means：
1.在 K-means 算法中 K 是事先给定的，这个 K 值的选定是非常难以估计的，如果分类明显那么此算法很高效  
2.此算法中需要给定个初始值，这个值得选择对于后续有很大影响  
3.不断迭代，在大数据上性能比较低  
先随机选取K个点，得到分类后计算中心，在计算各个点到中心的距离重新分类，一直重复直到收敛  
不收敛的情况下，在误差小于多少的情况下结束  

### BP神经网络：
1.最小误差，采用梯度下降方法  
2.学习速度固定，比较慢  
3.输入层，隐含层，输出层  
4。非线性变换  
不断调整权重值来看计算值与期望值差异(权重，阈值，传递函数)  
在找不到线性或非线性函数式用神经网络  
属于有教师的学习，可以根据结果来调整权，  
神经网络可用于分类，回归，聚类  

### SOM神经网络：
无教师学习（有无期望输出）
自组织竞争神经网络（只有一个神经元胜出）
用低维度点来表示高维度的数据
主要用来模式识别和聚类

### 双变量线性回归：
y=a*X（b指数） 
属于分类算法  

### 双变量自然指数回归
y = aln(x) + b

R2拟合度检验，在0-1之间，越大越好，最小二乘分
F整体性检验，方差等，

### C4.5决策树
1.用信息熵判断
2.可以处理连续和离散型值
3.分类精度高
4.对噪声有很好的健壮性

决策树需要修剪，防止过度训练，在样本上完全符合，但实际数据中表现很差，如限定层次，限定几点包含最少记录数
可以处理离散值和连续值

### CHAID决策树：
卡方自动交互检测

CHAID、CART和C4.5大概是决策树算法丛林中最有名，商业上运用也也最成功的算法了。CHAID (chi-squared automatic interaction detection，卡方自动交互检测)的前身是AID，主要特征是多向分叉，前向修剪，其标准如名所示，就是卡方检测；另外，CHAID只能处理类别型的输入变量，因此连续型的输入变量首先要进行离散处理。标准的CART(classification and regression trees)又不一样，它只能进行二部分叉，后向修剪，分割标准用的是基尼系数(Gini Index)；C4.5源自有名的ID3，它只能进行L型分叉，后向修剪，标准乃是基于信息论的“熵”(Entropy)。

### 时间序列：
移动平均法：如计算当前向前数十个月的均值，然后根据这个趋势来预测
一次指数平滑法：为加权重预测，权数为a, 即历史数据所占比重， 需要测试并计算方差找到最佳值，预测值为前一期的预测值奸商误差的修正值
二次指数平滑法：
三次指数平滑法：
